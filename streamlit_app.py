import streamlit as st
from PIL import Image
from io import BytesIO
import base64
import requests
import openai
from transformers import AutoImageProcessor, ViTForImageClassification
import torch
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain_core.documents import Document
from Bio import Entrez

Image.MAX_IMAGE_PIXELS = None

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "qa_history" not in st.session_state:
    st.session_state.qa_history = []

# LLM RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë‹¹ì‹ ì€ ë‚œì†Œì•” ë³‘ë¦¬ ì´ë¯¸ì§€ë¥¼ í•´ì„í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ë…¼ë¬¸ì—ì„œ ë°œì·Œí•œ ë‚´ìš©ì…ë‹ˆë‹¤:
---------------------
{context}
---------------------

ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸: {question}
"""
)

# PubMed Boolean Query ìƒì„±
def generate_pubmed_query_from_question(question: str, llm=None) -> str:
    if llm is None:
        llm = ChatOpenAI(model="gpt-4", temperature=0, api_key=st.session_state.get("user_api_key", ""))
    query_prompt = PromptTemplate(
        input_variables=["question"],
        template="""ì•„ë˜ ì„¤ëª…ì€ ë³‘ë¦¬ ì´ë¯¸ì§€ AI ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.
ì´ ì„¤ëª…ì— ë§ëŠ” PubMed ê²€ìƒ‰ Boolean ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì„¤ëª…:
{question}

ì¶œë ¥ í˜•ì‹: Boolean Query
"""
    )
    return llm.invoke(query_prompt.format(question=question)).content.strip()

# PubMed ê²€ìƒ‰
def search_pubmed(question: str, max_results: int = 3):
    Entrez.email = "teamovianai@gmail.com"
    query = question  # LLM Boolean query ì•ˆ ì“°ê³  ê·¸ëƒ¥ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results)
    record = Entrez.read(handle)
    ids = record["IdList"]
    summaries = []

    for pmid in ids:
        summary = Entrez.esummary(db="pubmed", id=pmid, retmode="xml")
        summary_record = Entrez.read(summary)
        title = summary_record[0].get("Title", "[ì œëª© ì—†ìŒ]")
        pubdate = summary_record[0].get("PubDate", "")
        year = pubdate.split()[0] if pubdate else ""
        authors = summary_record[0].get("AuthorList", [])
        author_str = ", ".join(authors[:2]) + (" et al." if len(authors) > 2 else "")
        fetch = Entrez.efetch(db="pubmed", id=pmid, rettype="abstract", retmode="text")
        abstract = fetch.read()

        summaries.append(Document(
            page_content=abstract,
            metadata={"pmid": pmid, "title": title, "year": year, "authors": author_str, "source": "pubmed"}
        ))
    return summaries

# ì´ë¯¸ì§€ ê¸°ë°˜ ë…¼ë¬¸ ê²€ìƒ‰ (ì˜ˆì‹œ ViT ì‚¬ìš©)
def find_similar_papers_from_image(image_bytes):
    image = Image.open(BytesIO(image_bytes))
    processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')
    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')

    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
    return [
        Document(page_content="Ovarian cancer tissue with high mitotic index...", metadata={"pmid": "12345678", "source": "pubmed"}),
        Document(page_content="Study on serous subtype of ovarian carcinoma...", metadata={"pmid": "87654321", "source": "pubmed"})
    ]

# âœ… Streamlit UI
st.set_page_config(page_title="Ovarian Cancer RAG", layout="wide")

st.sidebar.title("ğŸ” OpenAI API KEY ì…ë ¥")
user_api_key = st.sidebar.text_input("API Key", type="password")
if user_api_key:
    st.session_state["user_api_key"] = user_api_key
openai.api_key = st.session_state.get("user_api_key", "")

st.title("ğŸ”¬ ë‚œì†Œì•” ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸")

question = st.text_input("í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆì‹œ: What are the subtypes of ovarian cancer?")
uploaded_file = st.file_uploader("ì¡°ì§ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["png", "jpg", "jpeg"])

if st.button("ë¶„ì„ ì‹¤í–‰"):
    llm = ChatOpenAI(model="gpt-4", temperature=0, api_key=st.session_state.get("user_api_key", ""))
    embeddings = HuggingFaceEmbeddings(model_name="dmis-lab/biobert-base-cased-v1.1")

    if uploaded_file is not None:
        image_bytes = uploaded_file.read()
        st.image(Image.open(BytesIO(image_bytes)), caption="ì—…ë¡œë“œëœ ì¡°ì§ ì´ë¯¸ì§€", use_container_width=True)

        try:
            with st.spinner("1ï¸âƒ£ ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° Flask ì „ì†¡ ì¤‘..."):
                files = {'file': (uploaded_file.name, image_bytes)}
                response = requests.post("http://localhost:5001/infer", files=files)

            if response.status_code == 200:
                result = response.json()
                pred_class = result["pred_class"]
                softmax_probs = result["softmax_probs"]
                attention_base64 = result["attention_map_base64"]

                st.success("âœ… Flask ì„œë²„ ì¶”ë¡  ì™„ë£Œ!")

                with st.spinner("2ï¸âƒ£ Attention Map ì‹œê°í™” ì¤‘..."):
                    st.image(BytesIO(base64.b64decode(attention_base64)), caption="AI Attention Map", use_container_width=True)

                label_dict = {0: 'HGSC', 1: 'LGSC', 2: 'CC', 3: 'EC', 4: 'MC'}

                st.success(f"âœ… AI ì˜ˆì¸¡ í´ë˜ìŠ¤: {pred_class} ({label_dict[pred_class]})")

                probs_percent = [
                    f"{label_dict[i]}: {int(p * 100)}%"
                    for i, p in enumerate(softmax_probs)
                ]

                st.write("Softmax Probabilities:")
                for line in probs_percent:
                    st.write(f"- {line}")

                with st.spinner("3ï¸âƒ£ PubMed ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..."):
                    ai_description = f"Predicted class: {label_dict[pred_class]}, softmax: {probs_percent}"
                    related_papers = search_pubmed(ai_description, max_results=3)

                st.success("âœ… PubMed ê²€ìƒ‰ ì™„ë£Œ!")
                # âœ… uploads í´ë” ë¹„ìš°ê¸°
                try:
                    clear_response = requests.post("http://localhost:5001/clear_uploads")
                    if clear_response.status_code == 200:
                        st.success("âœ… ì„œë²„ uploads í´ë” ì •ë¦¬ ì™„ë£Œ!")
                    else:
                        st.warning(f"âš ï¸ uploads í´ë” ì •ë¦¬ ì‹¤íŒ¨: {clear_response.text}")
                except Exception as e:
                    st.warning(f"âš ï¸ Flask ì„œë²„ ì •ë¦¬ ìš”ì²­ ì‹¤íŒ¨: {e}")

                

                st.subheader("ğŸ“„ PubMed ê´€ë ¨ ë…¼ë¬¸")
                for doc in related_papers:
                    title = doc.metadata.get("title", "[ì œëª© ì—†ìŒ]")
                    year = doc.metadata.get("year", "")
                    authors = doc.metadata.get("authors", "")
                    pmid = doc.metadata.get("pmid", "")
                    url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else ""
                    st.markdown(f"- [{title}]({url}) ({year}, {authors})" if url else f"- {title} ({year}, {authors})")

            else:
                st.error(f"âŒ Flask ì¶”ë¡  ì—ëŸ¬: {response.text}")


        except Exception as e:
            st.error(f"ğŸš¨ Flask ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")

    elif question:
        docs = search_pubmed(question=question)
        splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=200)
        chunks = splitter.split_documents(docs)
        vector_db = FAISS.from_documents(chunks, embeddings)
        retriever = vector_db.as_retriever(search_kwargs={"k": 3})

        rag_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever,
            chain_type="stuff",
            chain_type_kwargs={"prompt": prompt_template},
            return_source_documents=True
        )
        result = rag_chain.invoke({"query": question})

        st.session_state.qa_history.insert(0, {
            "question": question,
            "answer": result["result"],
            "sources": result["source_documents"]
        })

        st.subheader("ğŸ–ï¸ ìš”ì•½ ë‹µë³€")
        st.write(result["result"])
        st.subheader("ğŸ“„ ë…¼ë¬¸ ì¶œì²˜")
        for doc in result["source_documents"]:
            pmid = doc.metadata.get("pmid")
            title = doc.metadata.get("title", "[ì œëª© ì—†ìŒ]")
            year = doc.metadata.get("year", "")
            authors = doc.metadata.get("authors", "")
            url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else ""
            st.markdown(f"- [{title}]({url}) ({year}, {authors})" if url else f"- {title} ({year}, {authors})")

# âœ… Q&A íˆìŠ¤í† ë¦¬ í‘œì‹œ
if len(st.session_state.qa_history) > 1:
    st.markdown("## ğŸ“š ì´ì „ Q&A ê¸°ë¡")
    for idx, entry in enumerate(st.session_state.qa_history[1:]):
        with st.expander(f"Q{len(st.session_state.qa_history) - idx - 1}: {entry['question']}"):
            st.write(entry["answer"])
            for doc in entry["sources"]:
                pmid = doc.metadata.get("pmid")
                title = doc.metadata.get("title", "[ì œëª© ì—†ìŒ]")
                year = doc.metadata.get("year", "")
                authors = doc.metadata.get("authors", "")
                url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else ""
                st.markdown(f"- [{title}]({url}) ({year}, {authors})" if url else f"- {title} ({year}, {authors})")

# âœ… ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ê¸°ë¡ ì´ˆê¸°í™”"):
    st.session_state.qa_history = []
    st.rerun()
